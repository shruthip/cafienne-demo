include "application"

akka {
  loglevel = DEBUG
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 10s

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    serialize-messages = on

    serializers {
      serializer = "org.cafienne.cmmn.akka.serialization.CafienneSerializer"
      // offset_serializer is used to serialize offset snapshots
      offset_serializer = "org.cafienne.infrastructure.eventstore.OffsetSerializer"
    }
    serialization-bindings {
      "org.cafienne.cmmn.instance.CaseInstanceEvent" = serializer
      // Current offsets are WrappedOffset objects
      "org.cafienne.infrastructure.eventstore.WrappedOffset" = offset_serializer
      // enable below to check if all events have been serialized without java.io.Serializable
      //"java.io.Serializable" = none
    }
  }

  debug {
    receive = off
    unhandled = on
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    seed-nodes = ["akka.tcp://ClusterSystem@127.0.0.1:2551"]
    #roles = ["case-domain"]
    auto-down-unreachable-after = 30s
  }

  persistence {
    journal {
      # DO NOT USE LEVELDB FOR A MULTI NODE SETUP !!!
      # NOTE: Default journal is leveldb, as it comes out of the box without setup.
      # However, this cannot be used in production or in a multi-node setup.
      # In that case, the cassandra-journal has to be enabled.
      #plugin = "akka.persistence.journal.leveldb"
      plugin = "cassandra-journal"

      # Default configuration for leveldb storage of events.
      # Cassandra configuration is at the end of this file
      leveldb {
        store {
          # DO NOT USE 'native = off' IN PRODUCTION !!!
          native = off
          dir = "journal"
        }

        event-adapters {
          tagging = "org.cafienne.cmmn.instance.journal.CaseTaggingEventAdapter"
        }

        event-adapter-bindings {
          "org.cafienne.cmmn.instance.CaseInstanceEvent" = tagging
        }
      }
    }

    snapshot-store {
      # Path to the snapshot store plugin to be used
      plugin = cassandra-snapshot-store
      #plugin = "akka.persistence.snapshot-store.local"

      # Local filesystem snapshot store plugin.
      local {

        # Class name of the plugin.
        class = "akka.persistence.snapshot.local.LocalSnapshotStore"

        # Dispatcher for the plugin actor.
        plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"

        # Dispatcher for streaming snapshot IO.
        stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"

        # Storage location of snapshot files.
        dir = "snapshots"
      }
    }
  }

  cluster {
    sharding {
      # The extension creates a top level actor with this name in top level user scope,
      # e.g. '/user/sharding'
      guardian-name = sharding
      # If the coordinator can't store state changes it will be stopped
      # and started again after this duration.
      coordinator-failure-backoff = 10 s
      # Start the coordinator singleton manager on members tagged with this role.
      # All members are used if undefined or empty.
      # ShardRegion actor is started in proxy only mode on nodes that are not tagged
      # with this role.

      #role = "case-domain"
      #role = "domain"

      # The ShardRegion retries registration and shard location requests to the
      # ShardCoordinator with this interval if it does not reply.
      retry-interval = 2 s
      # Maximum number of messages that are buffered by a ShardRegion actor.
      buffer-size = 100000
      # Timeout of the shard rebalancing process.
      handoff-timeout = 60 s
      # Rebalance check is performed periodically with this interval.
      rebalance-interval = 10 s
      # How often the coordinator saves persistent snapshots, which are
      # used to reduce recovery times
      snapshot-interval = 3600 s
      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        rebalance-threshold = 10
        # The number of ongoing rebalancing processes is limited to this number.
        max-simultaneous-rebalance = 3
      }
    }
  }
}

cafienne {

  # The case engine reads definitions as XML files from disk and/or the classpath.
  # The files are cached in-memory, based on their lastModified timestamp
  # (i.e., if you change a file on disk, the engine will reload it into the cache).
  # By default, the engine will read from the configured location. If the definitions file cannot be found
  # in this location, the engine will try to load it as a resource from the classpath, hence enabling to ship
  # fixed definitions in a jar file.
  definitions {
    location = "./definitions"
    cache {
      size = 100
    }
  }

  api {
    bindhost = "0.0.0.0"
    bindport = 18082

    security {
      shared-secret = "234#dsfDG6!KKl81m?r7BHa02"
      auth-header-name = "X-AUTH-CAFIENNE"

      jwt {
        # Either HMAC256 or RSA256 is supported
        algorithm = "HMAC256"
        issuerId = "cafienne"
        # For RSA 256, uncomment this
        # issuerId = "myOrg"
        # rsa-public-key = """
        #  -----BEGIN PUBLIC KEY-----
        #  MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAniNWA7mqcYblE8g94Qsq
        #  62KDZlS24f6MuOzOtT8EDNBkjTR4r99++d85pjTYGj7+vL8zkwNvC/oBZvpG0Th+
        #  V+UcYd7BaTbt1t/Xp58CTu1kgrMLvRX2rEr+jbYr6LedEbCrifp53/hV/b5Q/Utf
        #  j5l+eA0ezAyaeExrzL20xdg0S03D1K/aomjiggixqM4JTg0duE13Qlf3wJytTYoO
        #  tvAGLqLHmbr9QV+qAJCsrJuUnXqJa2YJ47ippKo1pwXl4t4yTxslBB0u/gO/ufex
        #  p4E6UHbX2KnKsGU7c+zSzkCskYIDNyPYLMlliA5UPUC/0+nLKGAjuNYxjCumMsT7
        #  2FbNwMNb6ho/QpdLUHt2SY6k4GmO2bXLjmgrcYIZLn/0xPpj5Tfuawtefr9wVBzy
        #  TSzi7sZQ9XRj2/0OVEqrAzJi1aTcnoMMMsQJ1EFeUiqnGMJpnK4wCxX8FVeGVWl9
        #  7FSmQp8Q7pPLgOGPbsFuVVPdCqVsBOkJoYSUDolUljPUBSN21qXSqXlwVbP+rk8Z
        #  hGra89+C4hrT9ys5KibxnT9HGEijqp2h1VKGGkm/B1e2zXNP411Ifn2jiY8yLdtI
        #  DCppjDt2Ht8tB18HTae/URSKWkEVhSxkT/3Ns1IUgZgrYitOTqnpQS7OsDTUXfcn
        #  HeGeHjxOfSKug26fGagHCVcCAwEAAQ==
        #  -----END PUBLIC KEY-----
        #  """
      }

      # Creates default users in Elastic Search upon startup, e.g. to be used as admin;
      #  Existing users will NOT be overwritten
      default-users = [{
        # id (user id) is mandatory, this is the login name
        id = "admin"
        # name is optional, if not given, id will be used
        name = "The administrator"
        # password is mandatory
        password = "cafienne"
        # Roles is an optional list of string, e.g. ["ADMIN", "MANAGER", "EMPLOYEE"]
        roles = ["ADMIN"]
        # Email is optional
        email = "admin@admin.com"
      },
      {
        id = "hank"
        name = "Hank"
        password = "hank"
        roles = ["Requestor"]
        email = "hank@travelatwork.com"
      },
      {
        id = "gerald"
        name = "Gerald"
        password = "gerald"
        roles = ["Requestor", "Approver"]
        email = "gerald@approversatwork.com"
      },
      {
        id = "suzy"
        name = "Suzy"
        password = "suzy"
        roles = ["Requestor", "PersonalAssistant"]
        email = "suzy@alwaysatwork.com"
      },
      {
        id = "lana"
        name = "Lana"
        password = "lana"
        roles = ["Requestor", "PersonalAssistant"]
        email = "lana@sometimesatwork.com"
	    }]
    }
  }

  elastic {
    # Settings for trying to reconnect to Elastic Search upon starting the case-service
    connect-retry {
      # Number of attempts we should try to connect to ElasticSearch; defaults to 6
      attempts = 6
      # Interval in milliseconds to wait inbetween connection attempts; defaults to 5000
      interval = 5000
    }
    host-list = ${ES_HOST}
  }

  elasticsearch {
    cluster.name = "cafienne"
    client.transport.ignore_cluster_name = true

    #client.transport.sniff = true
    #discovery.zen.ping.multicast.enabled = true
    #discovery.zen.ping.multicast.ping.enabled = false
    #discovery.zen.ping.unicast.hosts = ["80.79.203.63:9300"]
  }
}

cassandra-journal {
  event-adapters {
    tagging = "org.cafienne.cmmn.instance.journal.CaseTaggingEventAdapter"
  }

  event-adapter-bindings {
    "org.cafienne.cmmn.instance.CaseInstanceEvent" = tagging
  }

  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # Comma-separated list of contact points in the Cassandra cluster.
  # Host:Port pairs are also supported. In that case the port parameter will be ignored.
  contact-points = [${CS_HOST}]

  # Port of contact points in the Cassandra cluster.
  # Will be ignored if the contact point list is defined by host:port pairs.
  port = 9042

  # The implementation of akka.persistence.cassandra.SessionProvider
  # is used for creating the Cassandra Session. By default the 
  # the ConfigSessionProvider is building the Cluster from configuration properties
  # but it is possible to replace the implementation of the SessionProvider
  # to reuse another session or override the Cluster builder with other
  # settings.
  # For example, it is possible to lookup the contact points of the Cassandra cluster
  # asynchronously instead of giving them in the configuration in a subclass of
  # ConfigSessionProvider and overriding the lookupContactPoints method.
  # It may optionally have a constructor with an ActorSystem and Config parameter.
  # The config parameter is this config section of the plugin.
  session-provider = akka.persistence.cassandra.ConfigSessionProvider

  # The identifier that will be passed as parameter to the
  # ConfigSessionProvider.lookupContactPoints method. 
  cluster-id = ""

  # Name of the keyspace to be created/used by the journal
  keyspace = "akka"

  # Parameter indicating whether the journal keyspace should be auto created
  keyspace-autocreate = true

  # Parameter indicating whether the journal tables should be auto created
  tables-autocreate = true

  # The number of retries when a write request returns a TimeoutException or an UnavailableException.
  write-retries = 3

  # Deletes are achieved using a metadata entry and then the actual messages are deleted asynchronously
  # Number of retries before giving up
  delete-retries = 3

  # Number of retries before giving up connecting for the initial connection to the Cassandra cluster
  connect-retries = 3

  # Delay between connection retries, for the initial connection to the Cassandra cluster
  connect-retry-delay = 1s

  # Max delay of the ExponentialReconnectionPolicy that is used when reconnecting
  # to the Cassandra cluster
  reconnect-max-delay = 30s

  # Cassandra driver connection pool settings
  # Documented at https://datastax.github.io/java-driver/manual/pooling/
  connection-pool {

    # Create new connection threshold local
    new-connection-threshold-local = 800

    # Create new connection threshold remote
    new-connection-threshold-remote = 200

    # Connections per host core local
    connections-per-host-core-local = 1

    # Connections per host max local
    connections-per-host-max-local = 4

    # Connections per host core remote
    connections-per-host-core-remote = 1

    # Connections per host max remote
    connections-per-host-max-remote = 4

    # Max requests per connection local
    max-requests-per-connection-local = 32768

    # Max requests per connection remote
    max-requests-per-connection-remote = 2000

    # Sets the timeout when trying to acquire a connection from a host's pool
    pool-timeout-millis = 0
  }

  # Name of the table to be created/used by the journal.
  # If the table doesn't exist it is automatically created.
  table = "messages"

  # Compaction strategy for the journal table.
  # Please refer to the tests for example configurations.
  # Refer to http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compactSubprop.html 
  # for more information regarding the properties.
  table-compaction-strategy {
    class = "SizeTieredCompactionStrategy"
  }

  # Name of the table to be created/used for storing metadata.
  # If the table doesn't exist it is automatically created.
  metadata-table = "metadata"

  # Name of the table to be created/used for journal config.
  # If the table doesn't exist it is automatically created.
  config-table = "config"

  # Set this to on to only use Cassandra 2.x compatible features,
  # i.e. if you are using a Cassandra 2.x server.
  # To run tests with Cassandra 2.x server you have to do the following:
  # - start Cassandra 2.x server on default port 9042, with empty data directory
  # - change CassandraLauncher.randomPort to 9042
  # - change CassandraLauncher.start to do nothing
  # - set this cassandra-2x-compat = on
  # - note that you must delete all data between each test run
  cassandra-2x-compat = off

  # Possibility to disable the eventsByTag query and creation of
  # the materialized view. This will automatically be off when
  # cassandra-2x-compat=on 
  enable-events-by-tag-query = on

  # Name of the materialized view for eventsByTag query
  events-by-tag-view = "eventsbytag"

  # replication strategy to use. SimpleStrategy or NetworkTopologyStrategy
  replication-strategy = "SimpleStrategy"

  # Replication factor to use when creating a keyspace. Is only used when replication-strategy is SimpleStrategy.
  replication-factor = 1

  # Replication factor list for data centers, e.g. ["dc1:3", "dc2:2"]. Is only used when replication-strategy is NetworkTopologyStrategy.
  data-center-replication-factors = []

  # To limit the Cassandra hosts this plugin connects with to a specific datacenter.
  # (DCAwareRoundRobinPolicy withLocalDc)
  # The id for the local datacenter of the Cassandra hosts it should connect to. 
  # By default, this property is not set resulting in Datastax's standard round robin policy being used.
  local-datacenter = ""

  # Number of hosts from non-local datacenter to use as a fall-back policy.
  # Works only when local-datacenter is set
  used-hosts-per-remote-dc = 0

  # To connect to the Cassandra hosts with credentials.
  # Authentication is disabled if username is not configured.
  authentication.username = ""
  authentication.password = ""

  # SSL can be configured with the following properties.
  # SSL is disabled if the truststore is not configured.
  # For detailed instructions, please refer to the DataStax Cassandra chapter about 
  # SSL Encryption: http://docs.datastax.com/en/cassandra/2.0/cassandra/security/secureSslEncryptionTOC.html
  # Path to the JKS Truststore file 
  ssl.truststore.path = ""
  # Password to unlock the JKS Truststore
  ssl.truststore.password = ""
  # Path to the JKS Keystore file (optional config, only needed for client authentication)
  ssl.keystore.path = ""
  # Password to unlock JKS Truststore and access the private key (both must use the same password)
  ssl.keystore.password = ""

  # Write consistency level
  # The default read and write consistency levels ensure that persistent actors can read their own writes.
  # During normal operation, persistent actors only write to the journal, reads occur only during recovery.
  write-consistency = "QUORUM"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of messages that will be batched when using `persistAsync`. 
  # Also used as the max batch size for deletes.
  max-message-batch-size = 100

  # Target number of entries per partition (= columns per row).
  # Must not be changed after table creation (currently not checked).
  # This is "target" as AtomicWrites that span partition boundaries will result in bigger partitions to ensure atomicity.
  target-partition-size = 500000

  # Maximum size of result set
  max-result-size = 50001

  # Maximum size of result set during replay
  max-result-size-replay = 50001

  # The query journal to use when recoverying
  query-plugin = "cassandra-query-journal"

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "cassandra-plugin-default-dispatcher"

  # Dispatcher for potentially blocking tasks.
  blocking-dispatcher = "cassandra-plugin-blocking-dispatcher"

  # The time to wait before cassandra will remove the thombstones created for deleted entries.
  # cfr. gc_grace_seconds table property documentation on http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/tabProp.html
  gc-grace-seconds = 864000

  # When using more than one tag per event you have to configure know 
  # tags in this section to give each tag an identifier. When using 
  # only one tag per event the identifier is 1, automatically.
  # tagname = tagid
  # where tagid must be 1, 2, or 3. Max 3 tags per event is supported.
  # For example:
  #   BlogPosts = 1
  #   Announcement = 2
  #   Authors = 1
  # With those tag identifiers you can use BlogPosts and Announcement for a single event,
  # but you cannot combine BlogPosts and Authors, since they have the same tag identifier.  
  tags {
  }

  # Minimum time between publishing messages to DistributedPubSub to announce events for a specific tag have
  # been written. These announcements cause any ongoing getEventsByTag to immediately re-poll, rather than
  # wait. In order enable this feature, make the following settings:
  #  
  #    - enable clustering for your actor system
  #    - cassandra-journal.pubsub-minimum-interval = 1s              (send real-time announcements at most every sec)
  #    - cassandra-query-journal.eventual-consistency-delay = 0s     (so it immediately tries to show changes)
  #
  # Setting pubsub-minimum-interval to "off" will disable the journal sending these announcements. 
  pubsub-minimum-interval = off

  # Set the protocol version explicitly, should only be used for compatibility testing.
  # Supported values: 3, 4
  protocol-version = ""
}

# This configures the default settings for all CassandraReadJournal plugin
# instances in the system.
#
# If you use multiple plugin instances you need to create differently named 
# sections containing only those settings that shall be different from the defaults
# configured here, importing the defaults like so:
#
#   my-cassandra-query-journal = ${cassandra-query-journal}
#   my-cassandra-query-journal {
#     <settings...>
#   }
cassandra-query-journal {
  # Implementation class of the Cassandra ReadJournalProvider
  class = "akka.persistence.cassandra.query.CassandraReadJournalProvider"

  # Absolute path to the write journal plugin configuration section
  write-plugin = "cassandra-journal"

  # New events are retrieved (polled) with this interval.
  refresh-interval = 1s

  # How many events to fetch in one query (replay) and keep buffered until they
  # are delivered downstreams.
  max-buffer-size = 500

  # The fetch size of the Cassandra select statement
  # Value less or equal to 0 means max-result-size will be used
  # http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/Statement.html
  max-result-size-query = 250

  # Read consistency level
  read-consistency = "QUORUM"

  # Configure this to the day (yyyyMMdd) when the system was first started.
  # When offset 0L is used it will look for events from this day and forward.
  first-time-bucket = "20151120"

  # The returned event stream is ordered by the offset (timestamp), which corresponds
  # to the same order as the write journal stored the events, with inaccuracy due to clock skew
  # between different nodes. The same stream elements (in same order) are returned for multiple
  # executions of the query on a best effort basis. The query is using a Cassandra Materialized
  # View for the query and that is eventually consistent, so different queries may see different
  # events for the latest events, but eventually the result will be ordered by timestamp
  # (Cassandra timeuuid column). To compensate for the the eventual consistency the query is
  # delayed to not read the latest events, the duration of this delay is defined by this 
  # configuration property.
  # However, this is only best effort and in case of network partitions
  # or other things that may delay the updates of the Materialized View the events may be
  # delivered in different order (not strictly by their timestamp).  
  eventual-consistency-delay = 1s

  # If you use the same tag for all events for a `persistenceId` it is possible to get
  # a more strict delivery order than otherwise. This can be useful when all events of
  # a PersistentActor class (all events of all instances of that PersistentActor class)
  # are tagged with the same tag. Then the events for each `persistenceId` can be delivered
  # strictly by sequence number. If a sequence number is missing the query is delayed up 
  # to the configured `delayed-event-timeout` and if the expected event is still not 
  # found the stream is completed with failure. This means that there must not be any 
  # holes in the sequence numbers for a given tag, i.e. all events must be tagged
  # with the same tag. Set this property to for example 30s to enable this feature.
  # It is disabled by default.
  delayed-event-timeout = 0s

  # Dispatcher for the plugin actors.
  plugin-dispatcher = "cassandra-plugin-default-dispatcher"

}

cassandra-snapshot-store {

  # FQCN of the cassandra snapshot store plugin
  class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

  # Comma-separated list of contact points in the cluster
  contact-points = [${CS_HOST}]

  # Port of contact points in the cluster
  port = 9042

  # Name of the keyspace to be created/used by the snapshot store
  keyspace = "akka_snapshot"

  # Parameter indicating whether the snapshot keyspace should be auto created
  keyspace-autocreate = true

  # In case that schema creation failed you can define a number of retries before giving up.
  keyspace-autocreate-retries = 1

  # Number of retries before giving up connecting to the cluster
  connect-retries = 3

  # Delay between connection retries
  connect-retry-delay = 5s

  # Name of the table to be created/used by the snapshot store
  table = "snapshots"

  # Compaction strategy for the snapshot table
  table-compaction-strategy {
    class = "SizeTieredCompactionStrategy"
  }

  # Name of the table to be created/used for journal config
  config-table = "config"

  # Name of the table to be created/used for storing metadata
  metadata-table = "metadata"

  # replication strategy to use. SimpleStrategy or NetworkTopologyStrategy
  replication-strategy = "SimpleStrategy"

  # Replication factor to use when creating a keyspace. Is only used when replication-strategy is SimpleStrategy.
  replication-factor = 1

  # Replication factor list for data centers, e.g. ["dc1:3", "dc2:2"]. Is only used when replication-strategy is NetworkTopologyStrategy.
  data-center-replication-factors = []

  # Write consistency level
  write-consistency = "ONE"

  # Read consistency level
  read-consistency = "ONE"

  # Maximum number of snapshot metadata to load per recursion (when trying to
  # find a snapshot that matches specified selection criteria). Only increase
  # this value when selection criteria frequently select snapshots that are
  # much older than the most recent snapshot i.e. if there are much more than
  # 10 snapshots between the most recent one and selected one. This setting is
  # only for increasing load efficiency of snapshots.
  max-metadata-result-size = 10

  # Maximum size of result set
  max-result-size = 50001

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "cassandra-snapshot-store.default-dispatcher"

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }
}
